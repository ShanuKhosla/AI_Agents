{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a4296b3-334c-4c7f-a68e-b378a8700c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Standard Python module to interact with the operating system (e.g., env variables, file paths)\n",
    "\n",
    "from openai import OpenAI  # Official OpenAI client – used for making API calls to GPT-4, GPT-3.5, etc.\n",
    "\n",
    "from langgraph.graph import StateGraph, END  \n",
    "# LangGraph is used to define stateful graphs (multi-agent or workflow pipelines)\n",
    "# StateGraph defines nodes and transitions, END is used to mark terminal states\n",
    "\n",
    "from typing import TypedDict, Annotated, List\n",
    "# Built-in Python module for type hints\n",
    "# TypedDict helps define structured dictionary types\n",
    "# Annotated lets you attach metadata to types (useful in LangChain input/output schemas)\n",
    "\n",
    "import operator  \n",
    "# Built-in Python module providing functional equivalents of operators (like <, ==, etc.)\n",
    "\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "# These are message types in LangChain's core message passing system\n",
    "# Used for defining and processing chat-style inputs/outputs\n",
    "\n",
    "from langchain_openai import ChatOpenAI  \n",
    "# LangChain wrapper for OpenAI's chat models – lets you call GPT-4/3.5 with extra features like streaming\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults  \n",
    "# Community-contributed LangChain integration for Tavily (a web search tool for real-time search results)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "memory = SqliteSaver.from_conn_string(\"my_agent.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1323e785-e310-4d48-a322-55539d588c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "da63f506-6d4b-43e1-8bf7-c6012c03529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "token = os.getenv(\"GITHUB_TOKEN\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17fe3edf-2fb2-430c-ba3b-dc84140677b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=\"openai/gpt-4.1-mini\",\n",
    "    base_url=\"https://models.github.ai/inference\",\n",
    "    api_key=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "623e4cef-4607-4a6c-b00f-dcbea7caf2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay. \\\n",
    "Write such an outline for the user provided topic. Give an outline of the essay along with any relevant notes \\\n",
    "or instructions for the sections.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e70298a2-cc6f-4ab0-95c3-c1b25a96fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays.\\\n",
    "Generate the best essay possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
    "Utilize all the information below as needed: \n",
    "\n",
    "------\n",
    "\n",
    "{content}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "599da731-f71b-463f-8f12-cf1dbb2c94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4293bd7b-ad40-4713-ab8c-6b455c7aeed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following essay. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8a1ceb9a-120c-41da-8a82-8e28bcc57ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as outlined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "61bf1245-0b12-4f0c-86a0-b17c35efa0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ced5f58d-40b4-48fd-aee6-7f865c39588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "tavily = TavilyClient(api_key=tavily_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f9a13d31-0a15-4e6c-9ff0-2b8fc210fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"plan\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5f054219-112b-4a70-8f24-27ff7343e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_plan_node(state: AgentState):\n",
    "    # Step 1: Generate research queries based on the task using an AI model\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),  # System instruction: how to generate queries\n",
    "        HumanMessage(content=state['task'])           # User-provided task, e.g., \"Research on AI safety\"\n",
    "    ])\n",
    "\n",
    "    # Step 2: Get existing content if available, else initialize an empty list\n",
    "    content = state['content'] or []\n",
    "\n",
    "    # Step 3: For each generated query, perform a web search and collect results\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)  # Search using Tavily API, limit to 2 results\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])  # Add the text content of each result to the list\n",
    "\n",
    "    # Step 4: Return the collected content in a dictionary (used as updated agent state)\n",
    "    return {\"content\": content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e40624ae-0ae5-4bea-8b80-74b1e05c214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_node(state: AgentState):\n",
    "    # Step 1: Combine all the collected content into a single string with line breaks\n",
    "    content = \"\\n\\n\".join(state['content'] or [])\n",
    "\n",
    "    # Step 2: Create a message from the user with the task and their current plan\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Prepare the list of messages to send to the model\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=WRITER_PROMPT.format(content=content)  # Fill in the prompt with the researched content\n",
    "        ),\n",
    "        user_message  # Include the user's task and plan\n",
    "    ]\n",
    "\n",
    "    # Step 4: Invoke the model with the prepared messages to generate a draft\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # Step 5: Return the generated draft and update the revision number\n",
    "    return {\n",
    "        \"draft\": response.content,  # The AI-generated draft\n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1  # Increment revision number (starts at 1 if not present)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8efecc3a-8c11-439e-845c-2b60a4223fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT), \n",
    "        HumanMessage(content=state['draft'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"critique\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0cb0af6a-0fff-4a5f-865d-e0e06a9dbf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_critique_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "        HumanMessage(content=state['critique'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a67a7de8-fc09-443b-b3bd-bf5c289e7b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    if state[\"revision_numner\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "705c1d64-da31-4273-9c93-653af9ee2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a2791e5f-8b31-4f24-9beb-ee375af3a4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1e60fabf910>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7eaedf6e-1091-403e-9723-da516d7b54b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1e60fabf910>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.set_entry_point(\"planner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b8a6393-c8f0-4214-ac98-c243341aba2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1e60fabf910>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    should_continue,\n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2cc906d9-bbf8-44d4-bb5a-f229ea9c9fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1e60fabf910>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1311b87f-b5fe-45a4-b42f-244a1c02c1b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_GeneratorContextManager' object has no attribute 'get_next_version'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m app \u001b[38;5;241m=\u001b[39m graph\n\u001b[0;32m      5\u001b[0m thread \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream({\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the difference between langchain and langsmith\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_revisions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     10\u001b[0m }, thread):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(s)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\__init__.py:2466\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_during \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2465\u001b[0m     config[CONF][CONFIG_KEY_CHECKPOINT_DURING] \u001b[38;5;241m=\u001b[39m checkpoint_during\n\u001b[1;32m-> 2466\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mSyncPregelLoop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2467\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStreamProtocol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_modes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpointer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2475\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2476\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_channels_asis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2478\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_during\u001b[49m\n\u001b[0;32m   2482\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   2483\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCONF\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG_KEY_CHECKPOINT_DURING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrigger_to_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger_to_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmigrate_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_migrate_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m loop:\n\u001b[0;32m   2489\u001b[0m     \u001b[38;5;66;03m# create runner\u001b[39;00m\n\u001b[0;32m   2490\u001b[0m     runner \u001b[38;5;241m=\u001b[39m PregelRunner(\n\u001b[0;32m   2491\u001b[0m         submit\u001b[38;5;241m=\u001b[39mconfig[CONF]\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   2492\u001b[0m             CONFIG_KEY_RUNNER_SUBMIT, weakref\u001b[38;5;241m.\u001b[39mWeakMethod(loop\u001b[38;5;241m.\u001b[39msubmit)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2495\u001b[0m         node_finished\u001b[38;5;241m=\u001b[39mconfig[CONF]\u001b[38;5;241m.\u001b[39mget(CONFIG_KEY_NODE_FINISHED),\n\u001b[0;32m   2496\u001b[0m     )\n\u001b[0;32m   2497\u001b[0m     \u001b[38;5;66;03m# enable subgraph streaming\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\loop.py:930\u001b[0m, in \u001b[0;36mSyncPregelLoop.__init__\u001b[1;34m(self, input, stream, config, store, cache, checkpointer, nodes, specs, trigger_to_nodes, manager, interrupt_after, interrupt_before, input_keys, output_keys, stream_keys, migrate_checkpoint, retry_policy, cache_policy, checkpoint_during)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack \u001b[38;5;241m=\u001b[39m ExitStack()\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpointer:\n\u001b[1;32m--> 930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_get_next_version \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpointer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_version\u001b[49m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_put_writes \u001b[38;5;241m=\u001b[39m checkpointer\u001b[38;5;241m.\u001b[39mput_writes\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_put_writes_accepts_task_path \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    933\u001b[0m         signature(checkpointer\u001b[38;5;241m.\u001b[39mput_writes)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_path\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    935\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_GeneratorContextManager' object has no attribute 'get_next_version'"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "with SqliteSaver.from_conn_string(\"my_agent.db\") as checkpointer:\n",
    "    app = graph\n",
    "    thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for s in app.stream({\n",
    "        'task': \"what is the difference between langchain and langsmith\",\n",
    "        \"max_revisions\": 2,\n",
    "        \"revision_number\": 1,\n",
    "    }, thread):\n",
    "        print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca544c61-5751-4d42-8065-c1315452b2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7babd-40c2-4ebc-944b-246b1eb52ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
