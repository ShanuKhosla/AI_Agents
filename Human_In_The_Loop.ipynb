{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5e2ffe80-b1a3-46c4-b5da-cc784c77d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Standard Python module to interact with the operating system (e.g., env variables, file paths)\n",
    "\n",
    "from openai import OpenAI  # Official OpenAI client â€“ used for making API calls to GPT-4, GPT-3.5, etc.\n",
    "\n",
    "from langgraph.graph import StateGraph, END  \n",
    "# LangGraph is used to define stateful graphs (multi-agent or workflow pipelines)\n",
    "# StateGraph defines nodes and transitions, END is used to mark terminal states\n",
    "\n",
    "from typing import TypedDict, Annotated  , Literal\n",
    "# Built-in Python module for type hints\n",
    "# TypedDict helps define structured dictionary types\n",
    "# Annotated lets you attach metadata to types (useful in LangChain input/output schemas)\n",
    "\n",
    "import operator  \n",
    "# Built-in Python module providing functional equivalents of operators (like <, ==, etc.)\n",
    "\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "# These are message types in LangChain's core message passing system\n",
    "# Used for defining and processing chat-style inputs/outputs\n",
    "\n",
    "from langchain_openai import ChatOpenAI  \n",
    "# LangChain wrapper for OpenAI's chat models â€“ lets you call GPT-4/3.5 with extra features like streaming\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults  \n",
    "# Community-contributed LangChain integration for Tavily (a web search tool for real-time search results)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from uuid import uuid4\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "51002706-5156-4e07-b4ae-35e222dde932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "token = os.getenv(\"GITHUB_TOKEN\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "endpoint = \"https://models.github.ai/inference\"\n",
    "model_name = \"microsoft/Phi-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0951e0b3-b505-4101-8798-978ad3b2b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In previous examples we've annotated the `messages` state key\n",
    "# with the default `operator.add` or `+` reducer, which always\n",
    "# appends new messages to the end of the existing messages array.\n",
    "\n",
    "# Now, to support replacing existing messages, we annotate the\n",
    "# `messages` key with a customer reducer function, which replaces\n",
    "# messages with the same `id`, and appends them otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0d8f19fa-14a7-4a8c-b049-3a659e4148bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "def merge_messages(old: list[AnyMessage], new: list[AnyMessage]) -> list[AnyMessage]:\n",
    "    for msg in new:\n",
    "        if not msg.id:\n",
    "            msg.id = str(uuid4()) # Assign unique ID if missing\n",
    "    return old + new  # Combine old and new messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2f7cfcf7-a92d-443a-bf3d-77a18a6ff026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgesntState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], merge_messages]\n",
    "    approved: bool\n",
    "    retry_count: int\n",
    "    rejections: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "95a9d933-05e6-4a59-abbb-eea24dde6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=\"microsoft/Phi-4\",\n",
    "    base_url=\"https://models.github.ai/inference\",\n",
    "    api_key=token,\n",
    "    streaming = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "723bfa98-9245-44ab-8121-dd022d203e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# âœ… Node 1: Call the LLM and generate a response\n",
    "# --------------------------------------\n",
    "def suggest_answer(state: AgentState):\n",
    "     # Extract message history\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Call the model to generate a reply\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # Return new message to be merged into state\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4b1d95f9-8521-49ae-8872-804a8389d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# âœ… Node 2: Human-in-the-loop approval step\n",
    "# --------------------------------------\n",
    "def human_approval(state: AgentState):\n",
    "    # Extract latest AI response from msg list\n",
    "    latest_ai_msg = state[\"messages\"][-1].content\n",
    "\n",
    "    # Display it to human\n",
    "    print(\"\\n LLM Suggested:\")\n",
    "    print(latest_ai_msg)\n",
    "\n",
    "    #Ask for approval from the human\n",
    "    decision = input(\"Approve this response? (y/n): \").strip().lower()\n",
    "\n",
    "    approved = decision == \"y\"\n",
    "\n",
    "    return {\"Approved\": approved}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3d766ef5-3c30-4164-a412-e84f2889848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# âœ… Define the LangGraph workflow\n",
    "# --------------------------------------\n",
    "graph = StateGraph(AgentState)\n",
    "# Add the LLM node\n",
    "graph.add_node(\"llm\", suggest_answer)\n",
    "\n",
    "# Add the human approval node\n",
    "graph.add_node(\"hitl\", human_approval)\n",
    "\n",
    "# Set the entry point of the graph to be the LLM\n",
    "graph.set_entry_point(\"llm\")\n",
    "\n",
    "# After LLM responds, go to the HITL node\n",
    "graph.add_edge(\"llm\", \"hitl\")\n",
    "\n",
    "# After human approval:\n",
    "# - If approved, go to END\n",
    "# - If rejected, go back to the LLM to retry\n",
    "graph.add_conditional_edges(\n",
    "    \"hitl\",                        # Node to check condition from\n",
    "    lambda state: state[\"approved\"],  # Condition function (True/False)\n",
    "    {\n",
    "        True: END,                # End graph if approved\n",
    "        False: \"llm\",             # Retry LLM if rejected\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph into a runnable workflow\n",
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3b449333-add8-488e-8336-b8884e7f9597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ Ask something:  Whats up?\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# âœ… Kick off the process with user input\n",
    "# --------------------------------------\n",
    "# Ask the human for a question\n",
    "user_input = input(\"ðŸ‘¤ Ask something: \")\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=user_input)],\n",
    "    \"approved\": False,\n",
    "    \"retry_count\": 0,      # â† Needed for loop control if you're using retry limits\n",
    "    \"rejections\": []       # â† Needed if you're storing rejected attempts\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8be130e7-aaed-49c3-93ec-1944547ddbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LLM Suggested:\n",
      "Iâ€™m here to help if you have any questions or need assistance! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Approve this response? (y/n):  y\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'approved' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --------------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# âœ… Run the workflow (stream = step-by-step execution)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# --------------------------------------\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m workflow\u001b[38;5;241m.\u001b[39mstream(initial_state):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m step\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;66;03m#Print all AI Messages returned at this step\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\__init__.py:2533\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2531\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[0;32m   2532\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 2533\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   2534\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[0;32m   2535\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   2536\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   2537\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[0;32m   2538\u001b[0m ):\n\u001b[0;32m   2539\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[0;32m   2541\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[0;32m   2542\u001b[0m     )\n\u001b[0;32m   2543\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[0;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\pregel\\retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\utils\\runnable.py:625\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    624\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 625\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 377\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\graph\\branch.py:169\u001b[0m, in \u001b[0;36mBranch._route\u001b[1;34m(self, input, config, reader, writer)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 169\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finish(writer, \u001b[38;5;28minput\u001b[39m, result, config)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langgraph\\utils\\runnable.py:370\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 370\u001b[0m         ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    372\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "Cell \u001b[1;32mIn[132], line 22\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     15\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_edge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhitl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# After human approval:\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# - If approved, go to END\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# - If rejected, go back to the LLM to retry\u001b[39;00m\n\u001b[0;32m     20\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_conditional_edges(\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhitl\u001b[39m\u001b[38;5;124m\"\u001b[39m,                        \u001b[38;5;66;03m# Node to check condition from\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m state: state[\u001b[43mapproved\u001b[49m],  \u001b[38;5;66;03m# Condition function (True/False)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     {\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m: END,                \u001b[38;5;66;03m# End graph if approved\u001b[39;00m\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m,             \u001b[38;5;66;03m# Retry LLM if rejected\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     }\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Compile the graph into a runnable workflow\u001b[39;00m\n\u001b[0;32m     30\u001b[0m workflow \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcompile()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'approved' is not defined"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# âœ… Run the workflow (stream = step-by-step execution)\n",
    "# --------------------------------------\n",
    "for step in workflow.stream(initial_state):\n",
    "    for key, value in step.items():\n",
    "        if key == \"messages\":\n",
    "            #Print all AI Messages returned at this step\n",
    "            for msg in value:\n",
    "                print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a13776-184d-42ae-894e-a099413b99a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
